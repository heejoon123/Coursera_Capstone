{
    "cells": [
        {
            "metadata": {
                "collapsed": true
            },
            "cell_type": "markdown",
            "source": "# Clustering of the Neighborhoods of Cities Popular in Media\nHeejoon Ahn  \nFebruary 20, 2021\n\n## 1. Introduction and Business Problem\n\nLondon and New York are two of the popular locations for tourism as of late for many fans of different media types. For London, people have traveled not only for sightseeing, but also for the famous locations from novels, movies, and television shows. Some popular examples include the original *Sherlock Holmes* stories, the television adaptation *Sherlock*, *Doctor Who*, *Harry Potter*,and many more. Especially with the widespread fame of *Sherlock Holmes, Doctor Who, and Harry Potter*, the entertainment industry in England has even further catered to generate events like the popular Escape Rooms based on these stories to bring in fans from across the globe to London. \n\nFor New York City, people have mostly traveled there for some of the sightseeing in Manhattan such as Times Square. However, the city itself has been displayed in several stories and movies. The recently most popular series taking place in New York City is Marvel's *Avengers* series and *Spiderman* series. Some of the earlier 2000's movies have been successful in bringing in people such as the *Devil Wear's Prada* movie and even the Christmas family movie *Elf*. \n\nWith the influence of such media and the fans, the tourism in both cities from influence of media sources cannot be discredited. With this in mind, the goal of this project is to generate a way to help tourists visiting both cities to choose their destinations depending on the experiences and services available in respective cities and its neighborhoods.\n\n## 2. Data Description\n\n### 2.1 London Data\nThe data for London was retrieved from two different sources. One was from Wikipedia's web page looking at the different areas within the city. We will be scraping the data from the website (https://en.wikipedia.org/wiki/List_of_areas_of_London). \n\nThis page provides the following sets of information in the relevant table we will be scraping:\n1. Neighborhood \n2. Borough\n3. Town \n4. Postal Code District\n\nHowever, the table that we will be scraping does lack the information regarding latitude and longitude, which we need to generate the maps for our clustering methods. To retrieve them, we utilize the **ArcGIS API**. ArcGIS Online allows users to connect data regarding people and locations with interactive maps. This API will be used to help retrieve the latitudes and longitudes for this dataset. \n\n### 2.2 New York Data\nThe data for New York City will be retrieved from the data provided by previous coursework that introduced the New York City clustering methods. Therefore, the retrieving of data will follow the code from the ungraded lab: **Segmenting and Clustering Neighborhoods in New York City**. The information that is provided from this dataset is \n\n1. Neighborhood\n2. Borough\n3. Latitude\n4. Longitude\n\n\n### 2.3 Foursquare API\nTo retrieve the information of the various venues within certain neighborhoods, we will be using the **Foursquare API**. Foursquare, being a location data provider, has information on venue names, locations, menus and more. For our purposes, we will be connecting to the API to retrieve the list of venues from all neighborhoods in respective cities. The radius that the API will be looking at was designated as 500 meters. \n\nThe data retrieved from Foursquare contained information of venues within a specified distance of the longitude and latitude of the postcodes. The information obtained per venue as follows:\n1. Neighbourhood : Name of the Neighbourhood\n2. Neighbourhood Latitude : Latitude of the Neighbourhood\n3. Neighbourhood Longitude : Longitude of the Neighbourhood\n4. Venue : Name of the Venue\n5. Venue Latitude : Latitude of Venue\n6. Venue Longitude : Longitude of Venue\n7. Venue Category : Category of Venue\n\n## 3. Methodology\nThe first step, before doing anything of interest, will be to load in the necessary packages to do our work. The packages are loaded as shown in the code below:\n\n```python\n# loading packages\nimport pandas as pd\nimport numpy as np\nimport requests\nimport json\nfrom geopy.geocoders import Nominatim\nfrom pandas.io.json import json_normalize\nfrom arcgis.geocoding import geocode\nfrom arcgis.gis import GIS\nimport matplotlib.cm as cm\nimport matplotlib.colors as colors\nimport folium\nfrom sklearn.cluster import KMeans\n```\n\nThe packages that have been loaded perform the following:\n\n* **Pandas**: hande data analysis\n* **Numpy**: handle data in a vectorized manner \n* **requests**: handle requests\n* **json**: handle JSON files \n* **Nominatim**: convert an address into latitude and longitude values\n* **json_normalize**: tranform JSON file into a pandas dataframe\n* **matplotlib**: help to generate the maps in our project. \n* **folium**: map rendering library\n* **KMeans**: The machine learning model that we are using.\n\nThe main approach was to first explore the cities individually, plot their maps to show the neighborhoods, build the models by clustering all of the similar neighborhoods together and finally plot the new maps with the clustered neighborhoods. Afterwards, the discussion of the findings is made. \n\n### 3.1 Data Collection \nTo start the data collection, the first step we implemented was to scrape the data required for London and New York City. For London, we required information including postal codes, neighborhoods, and boroughs. Because the information is mostly provided on the wikipedia page, we scraped using the following steps: \n\n```python\nlondon_wiki = \"https://en.wikipedia.org/wiki/List_of_areas_of_London\"\nlondon_url = requests.get(london_wiki)\nlondon_df = pd.read_html(london_url.text)\nlondon_df = london_df[1]\nlondon_df\n```\n\nFor New York City, the information was provided in a previous lab in this course, so the data was retrieved in a similar fashion to what was provided in the lab notebook. \n\n```python\n# retrieving NYC data from provided json file\n!wget -q -O 'newyork_data.json' https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DS0701EN-SkillsNetwork/labs/newyork_data.json\nprint('Data downloaded!')\n\nwith open('newyork_data.json') as json_data:\n    newyork_data = json.load(json_data)\n    \n# getting the neighborhoods data\nneighborhoods_data = newyork_data['features']\n\n# define the dataframe columns\ncolumn_names = ['Borough', 'Neighborhood', 'Latitude', 'Longitude'] \n\n# instantiate the dataframe\nneighborhoods = pd.DataFrame(columns=column_names)\n\n# fill the dataframe\nfor data in neighborhoods_data:\n    borough = neighborhood_name = data['properties']['borough'] \n    neighborhood_name = data['properties']['name']\n        \n    neighborhood_latlon = data['geometry']['coordinates']\n    neighborhood_lat = neighborhood_latlon[1]\n    neighborhood_lon = neighborhood_latlon[0]\n    \n    neighborhoods = neighborhoods.append({'Borough': borough,\n                                          'Neighborhood': neighborhood_name,\n                                          'Latitude': neighborhood_lat,\n                                          'Longitude': neighborhood_lon}, ignore_index=True)\n    \nneighborhoods.head()\n```\n\nOnce that is done, a the workflow will consist of data cleaning processes, mapping, and more before we utilize the K-means algorithm to help with the clustering of neighborhoods to help tackle our business problem."
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.7",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.7.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}